{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPscEW7boyXAzDPaZrZB2Jo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fidelisaboke/ml-crash-course/blob/main/reinforcement_learning/q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning using Q-Learning\n"
      ],
      "metadata": {
        "id": "lVDCykURy_3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required Libraries"
      ],
      "metadata": {
        "id": "2sqvTqX5j72P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "0UTOIzohzPYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Environment\n",
        "- A 2D structure repesenting the states"
      ],
      "metadata": {
        "id": "NHBREKMukEmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment\n",
        "rewards = np.array([\n",
        "    [-10, 1, 0],\n",
        "    [0, -10, 10]\n",
        "])"
      ],
      "metadata": {
        "id": "6RYUzOC7kLBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Q-table\n",
        "- The Q-table will store pairings of states-action and q-values"
      ],
      "metadata": {
        "id": "fJtyD6eoksnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 states, 4 actions\n",
        "Q_table = np.zeros((6, 4))\n",
        "\n",
        "# Each grid position is assigned to a state number\n",
        "state_map = {\n",
        "    (0, 0): 0,\n",
        "    (0, 1): 1,\n",
        "    (0, 2): 2,\n",
        "    (1, 0): 3,\n",
        "    (1, 1): 4,\n",
        "    (1, 2): 5\n",
        "}"
      ],
      "metadata": {
        "id": "Lpez3BJuk8l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-learning Algorithm Definition"
      ],
      "metadata": {
        "id": "kfY6sFxgmU40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Actions\n",
        "actions = {\n",
        "    0: (-1, 0),  # Up\n",
        "    1: (1, 0),   # Down\n",
        "    2: (0, 1),   # Right\n",
        "    3: (0, -1)   # Left\n",
        "}\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1     # Learning rate\n",
        "gamma = 0.9     # Discount factor\n",
        "epsilon = 0.1   # Exploration factor\n"
      ],
      "metadata": {
        "id": "0jeGeNlxmikh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Useful Helper Functions"
      ],
      "metadata": {
        "id": "U8N-ckdNwIiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_position(position):\n",
        "  \"\"\"Check if a position is within the grid.\"\"\"\n",
        "  return 0 <= position[0] < rewards.shape[0] and 0 <= position[1] < rewards.shape[1]\n",
        "\n",
        "def get_next_state(current_position, action):\n",
        "  \"\"\"Return the next state based on the current position and action.\"\"\"\n",
        "  move = actions[action]\n",
        "  next_position = (current_position[0] + move[0], current_position[1] + move[1])\n",
        "  if is_valid_position(next_position):\n",
        "      return next_position\n",
        "\n",
        "  return current_position"
      ],
      "metadata": {
        "id": "Cb8AkkBVwIJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning Training (Iterations)"
      ],
      "metadata": {
        "id": "4mpxDW0bxbNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPISODES = 20000\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "  # Set the start position, which is (0, 0)\n",
        "  position = (0, 0)\n",
        "\n",
        "  # Map the start position to a state index to be used in the Q-table\n",
        "  state = state_map[position]\n",
        "\n",
        "  # Initialise step count\n",
        "  step_count = 0\n",
        "\n",
        "  while True:\n",
        "    # Select a random action based on epsilon-greedy\n",
        "    if np.random.rand() < epsilon:\n",
        "      action = np.random.choice(list(actions.keys()))\n",
        "    else:\n",
        "      # Choose the best-known action (highest Q-value for current state)\n",
        "      action = np.argmax(Q_table[state])\n",
        "\n",
        "    # Obtain the new position, state and reward\n",
        "    new_position = get_next_state(position, action)\n",
        "    new_state = state_map[new_position]\n",
        "    reward = rewards[new_position]\n",
        "\n",
        "    # Update the Q-value using the Q-learning formula\n",
        "    Q_table[state, action] = Q_table[state, action] + alpha * (\n",
        "        reward + gamma * np.max(Q_table[new_state]) - Q_table[state, action]\n",
        "    )\n",
        "\n",
        "    # Update state, position, and increment step count\n",
        "    position = new_position\n",
        "    state = new_state\n",
        "    step_count += 1\n",
        "\n",
        "    # End the episode if the goal has been reached or too many steps taken\n",
        "    if reward == 10 or step_count >= 4:\n",
        "      break\n",
        "\n"
      ],
      "metadata": {
        "id": "0t7Lrnkfxaxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Q-values"
      ],
      "metadata": {
        "id": "RxNNiNX_xadh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Q-values\n",
        "print(\"Final Q-values:\")\n",
        "print(Q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OE0WjMy40Glx",
        "outputId": "3f094b0a-2aff-4ba1-f405-fdb4a07fc7b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Q-values:\n",
            "[[-1.          0.         10.         -1.        ]\n",
            " [10.         -1.          9.         -1.        ]\n",
            " [ 8.35386201 10.          8.61830234  9.01512626]\n",
            " [-1.14479951  0.         -1.21772458  0.        ]\n",
            " [10.         -0.98044297  8.33228183  0.        ]\n",
            " [ 0.          0.          0.          0.        ]]\n"
          ]
        }
      ]
    }
  ]
}